# from selenium import webdriver
# from selenium.webdriver.chrome.options import Options
# from selenium.webdriver.common.by import By
# from selenium.webdriver.support.ui import WebDriverWait
# from selenium.webdriver.support import expected_conditions as EC
# from bs4 import BeautifulSoup
# import time

# def scrape_facebook_with_popup_close_and_scroll(url, scroll_duration=10):
#     chrome_options = Options()
#     chrome_options.add_argument("--headless")
#     chrome_options.add_argument("--no-sandbox")
#     chrome_options.add_argument("--disable-dev-shm-usage")
#     chrome_options.add_argument("--disable-gpu")
#     chrome_options.add_argument("--window-size=1920,1080")
#     chrome_options.add_argument(
#         "user-agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
#     )
#     # Optional: if using Chromium on Debian
#     # chrome_options.binary_location = "/usr/bin/chromium"

#     driver = webdriver.Chrome(options=chrome_options)
#     try:
#         print(f"[+] Loading page: {url}")
#         driver.get(url)

#         # Wait for body to appear
#         WebDriverWait(driver, 10).until(
#             EC.presence_of_element_located((By.TAG_NAME, "body"))
#         )

#         # Try to find and click any close button (case-insensitive aria-label)
#         try:
#             close_button = driver.find_element(
#                 By.XPATH,
#                 '//div[@aria-label="Close" or @aria-label="close"] | '
#                 '//button[@aria-label="Close" or @aria-label="close"] | '
#                 '//*[@aria-label="Close" or @aria-label="close"]'
#             )
#             if close_button.is_displayed() and close_button.is_enabled():
#                 print("[+] Found and clicking 'Close' element...")
#                 close_button.click()
#                 time.sleep(1)  # brief pause after click
#         except Exception as e:
#             print("[-] No 'Close' popup found or failed to click:", str(e))

#         # Scroll down for `scroll_duration` seconds
#         print(f"[+] Scrolling for {scroll_duration} seconds to load content...")
#         start_time = time.time()
#         while time.time() - start_time < scroll_duration:
#             driver.execute_script("window.scrollBy(0, 1000);")
#             time.sleep(0.2)  # small delay between scrolls

#         # Get body HTML after scrolling
#         body_element = driver.find_element(By.TAG_NAME, "body")
#         body_html = body_element.get_attribute("outerHTML")

#         # Parse with BeautifulSoup and clean
#         soup = BeautifulSoup(body_html, "html.parser")
#         for tag in soup(["script", "style", "svg", "noscript", "header", "footer", "nav", "aside"]):
#             tag.decompose()

#         text = soup.get_text(separator=" ", strip=True)
#         return text

#     except Exception as e:
#         print(f"[!] Error during scraping: {e}")
#         return None
#     finally:
#         driver.quit()

# # === Usage ===
# if __name__ == "__main__":
#     # Replace with a PUBLIC Facebook Page URL
#     page_url = "https://www.facebook.com/Ghorerbazarbd.comm"

#     extracted_text = scrape_facebook_with_popup_close_and_scroll(
#         url=page_url,
#         scroll_duration=10
#     )

#     if extracted_text:
#         print("\n[+] Extracted Text (first 1500 characters):\n")
#         print(extracted_text[:1500] + "..." if len(extracted_text) > 1500 else extracted_text)
#     else:
#         print("Failed to extract content.")



from utils import analyze_facebook_lead
import json
import time
import pandas as pd
# results = []
# print("Loading json data")
# json_data = None
# with open("extracted_ads.json", "r") as data:
#      json_data = json.load(data)

# for ad in json_data["ads"]:
#         print(f"Analyzing: {ad['advertiser']}")

#         # Clean URL
#         fb_link = ad['advertiser_facebook_link']
#         pagename = fb_link.split("com/")[1].replace("/", "")
#         # Skip if no valid Facebook link
#         isNumber = pagename.isnumeric()
#         if not fb_link or "facebook.com" not in fb_link or isNumber:
#             lead_result = {"probability": 0, "service": None, "reasoning": "No valid Facebook link"}
#         else:
#             lead_result = analyze_facebook_lead(fb_link)
#             time.sleep(2)  # Be respectful to Groq + FB

#         # Clean reasoning text by removing unwanted characters
#         reasoning = lead_result['reasoning'].replace('"', '').replace('}\n', '').replace('```','\n').strip() if lead_result['reasoning'] else ''
        
#         # Combine original ad data + analysis
#         combined = {
#             "Advertiser": ad['advertiser'],
#             "Facebook Link": fb_link,
#             "Website Link": ad['advertiser_website_link'],
#             "Contact": ad['contact'],
#             "Library ID": ad['library_id'],
#             "Buy Probability (%)": lead_result['probability'],
#             "Recommended Service": lead_result['service'],
#             "Reasoning": reasoning
#         }
#         results.append(combined)
# # Create DataFrame
# df = pd.DataFrame(results)
# # sort by probability of conversion
# df = df.sort_values(by="Buy Probability (%)", ascending=False)

# # Save to CSV
# df.to_csv('analyzed_leads.csv', index=False, encoding='utf-8')
# print("Successfully analyzed every facebook page and sorted according to probability and saved in csv")

pages_to_test = [
    {
      "advertiser": "Dhaka Online Grocery Store",
      "advertiser_facebook_link": "https://www.facebook.com/dhakawholesalepricebd/",
      "advertiser_website_link": None,
      "library_id": "1869881600293572",
      "start_date": "2025-11-04",
      "active_time": "3 hrs",
      "content_preview": "ðŸ”¥ðŸ”¥ Offer Offer ðŸ”¥ðŸ”¥  Buy one get one free  #follwers #everyonehighlightsfollowerseveryonehighlightsfollowerseveryone",
      "contact": None,
      "delivery_cost_inside": None,
      "delivery_cost_outside": None
    },
    {
      "advertiser": "The Reading Cafe",
      "advertiser_facebook_link": "https://www.facebook.com/readingcafe.bookstore/",
      "advertiser_website_link": None,
      "library_id": "1907843143416638",
      "start_date": "2025-11-04",
      "active_time": "7 hrs",
      "content_preview": "Remember: Your response MUST contain: 1. A brief analysis (max 5 lines) 2. JSON data ONLY inside a ```json code block",
      "contact": None,
      "delivery_cost_inside": None,
      "delivery_cost_outside": None
    },
    {
      "advertiser": "The Reading Cafe",
      "advertiser_facebook_link": "https://www.facebook.com/readingcafe.bookstore/",
      "advertiser_website_link": "http://www.thereadingcafebd.com/",
      "library_id": "1907843143416638",
      "start_date": "2025-11-04",
      "active_time": "7 hrs",
      "content_preview": "ðŸ“š ð—›ð—”ð—Ÿð—™ ð—£ð—¥ð—œð—–ð—˜ ð—•ð—¢ð—¢ð—ž ð—¦ð—”ð—Ÿð—˜! ðŸ“š ð‘­ð‘³ð‘¨ð‘» 50% ð‘«ð‘°ð‘ºð‘ªð‘¶ð‘¼ð‘µð‘» ð‘¶ð‘µ ð‘¨ð‘³ð‘³ ð‘¶ð‘¹ð‘°ð‘®ð‘°ð‘µð‘¨ð‘³ & ð‘°ð‘´ð‘ð‘¶ð‘¹ð‘»ð‘¬ð‘« ð‘©ð‘¶ð‘¶ð‘²ð‘º! ðŸŽ‰ Dive into your next great read with our massive Half Price Sale â€” featuring ð‘­ð’Šð’„ð’•ð’Šð’ð’, ð‘µð’ð’-ð‘­ð’Šð’„ð’•ð’Šð’ð’, ð‘ªð’‰ð’Šð’ð’…ð’“ð’†ð’â€™ð’” ð‘©ð’ð’ð’Œð’”...",
      "contact": "+880-1738-963-670",
      "delivery_cost_inside": "None",
      "delivery_cost_outside": "None"
    },
    {
      "advertiser": "The Reading Cafe",
      "advertiser_facebook_link": "https://www.facebook.com/readingcafe.bookstore/",
      "advertiser_website_link": "http://www.thereadingcafebd.com",
      "library_id": "1328297535102482",
      "start_date": "2025-11-04",
      "active_time": "7 hrs",
      "content_preview": "ðŸ“š ð—›ð—”ð—Ÿð—™ ð—£ð—¥ð—œð—–ð—˜ ð—•ð—¢ð—¢ð—šð—” âœ… Dive into your next great read with our massive Half Price Sale â€” featuring ð‘­ð’Šð’„ð’•ð’Šð’ð’, ð‘µð’ð’-ð‘­ð’Šð’„ð’•ð’Šð’ð’, ð‘ªð’‰ð’Šð’ð’…ð’“ð’†ð’â€™ð’” ð‘©ð’ð’ð’Œð’”, ð‘©ð’ð’™ ð‘ºð’†ð’•ð’”...",
      "contact": "+880-1738-963-670",
      "delivery_cost_inside": "2% charge for card/bKash payments",
      "delivery_cost_outside": "advance required"
    },
    {
      "advertiser": "And Or",
      "advertiser_facebook_link": "https://www.facebook.com/andor.readingcafe/",
      "advertiser_website_link": "http://www.thereadingcafebd.com/",
      "library_id": "24978811731813725",
      "start_date": "2025-11-04",
      "active_time": "7 hrs",
      "content_preview": "ðŸ“š ð—›ð—”ð—Ÿð—™ ð—£ð—¥ð—œð—–ð—˜ ð—•ð—¢ð—¢ð—š ð—¦ð—”ð—Ÿð—˜! ðŸ“š ð‘­ð‘³ð‘¨ð‘» 50% ð‘«ð‘°ð‘ºð‘ªð‘¶ð‘¼ð‘µð‘» ð‘¶ð‘µ ð‘¨ð‘³ð‘³ ð‘¶ð‘¹ð‘°ð‘®ð‘°ð‘µð‘¨ð‘³ & ð‘°ð‘´ð‘·ð‘¶ð‘¹ð‘»ð‘¬ð‘« ð‘©ð‘¶ð‘¶ð‘²ð‘º! ðŸŽ‰ Dive into your next great read with our massive Half Price Sale â€” featuring... ",
      "contact": "+880-1738-963-670",
      "delivery_cost_inside": "Home Delivery Available All Over Bangladesh",
      "delivery_cost_outside": None
    },
    {
      "advertiser": "Time Machine BD",
      "advertiser_facebook_link": "https://www.facebook.com/timemachinetmbd/",
      "advertiser_website_link": None,
      "library_id": "2127259851350392",
      "start_date": "2025-11-04",
      "active_time": "9 hrs",
      "content_preview": "In-store: Up to 20% off exclusive deals. Online: Flat 10% off every order. Pre-order limited pieces before theyâ€™re gone...",
      "contact": None,
      "delivery_cost_inside": "COD Across Bangladesh",
      "delivery_cost_outside": None
    },
  
    {
      "advertiser": "Emptique",
      "advertiser_facebook_link": "https://www.facebook.com/emptique/",
      "advertiser_website_link": None,
      "library_id": "798603479612122",
      "start_date": "2025-11-02",
      "active_time": "Active",
      "content_preview": "ðð®ð¢ð¥ð ð­ð«ð®ð¬ð­ ð­ð¡ðž ð¬ð¢ð¦ð©ð¥ðž ð°ðšð² â€” ð¥ðžð­ ðƒð¡ðšð¤ðšâ€™ð¬ ð›ðžð¬ð­ ð«ðžð¬ð­ðšð®ð«ðšð§ð­ð¬ ð¡ð¨ð¬ð­ ð²ð¨ð®. Step inside Dhakaâ€™s most elegant restaurants with ð„ð¦ð©ð­ð¢ðªð®ðž â€” Bangladeshâ€™s first platform...",
      "contact": "01814-231316",
      "delivery_cost_inside": None,
      "delivery_cost_outside": None
    },
    {
      "advertiser": "Tori : à¦¤à¦°à§€",
      "advertiser_facebook_link": "https://www.facebook.com/toriclothing/",
      "advertiser_website_link": None,
      "library_id": "845133538461631",
      "start_date": "2025-11-02",
      "active_time": "Active",
      "content_preview": "ðŸª¡Premium Cotton ð—žð—”ð—˜ð—œð—”! ð—˜ð—œð——/ðŸ®ðŸ±\nSoft as a hug! Lightweight, breathable & all-day comfy. Crafted for comfort lovers! Grab yours & create magic! ð—¦ð˜ð—¼ð—°ð—¸ ð—¹ð—¶ð—ºð—¶ð˜ð—²ð—±!",
      "contact": None,
      "delivery_cost_inside": None,
      "delivery_cost_outside": None
    },
    {
      "advertiser": "India Shopping BD",
      "advertiser_facebook_link": "https://www.facebook.com/IndiaShoppingBD/",
      "advertiser_website_link": None,
      "library_id": "1357083399453057",
      "start_date": "2025-10-31",
      "active_time": "Active",
      "content_preview": "Shop for you and your loved ones availing great deals. We are taking pre-orders from Malaysia and India. ðŸ‡§ðŸ‡©ðŸ‡²ðŸ‡¾ðŸ‡®ðŸ‡³ Delivery time is 10-20 days...",
      "contact": None,
      "delivery_cost_inside": None,
      "delivery_cost_outside": None
    },
    {
      "advertiser": "Printacy",
      "advertiser_facebook_link": "https://www.facebook.com/printacy/",
      "advertiser_website_link": None,
      "library_id": "790084117343465",
      "start_date": "2025-11-01",
      "active_time": "Active",
      "content_preview": "Premium Shopping Bags â€“ à¦¯à§‡à¦–à¦¾à¦¨à§‡ à¦¸à§à¦Ÿà¦¾à¦‡à¦² à¦®à¦¿à¦¶à§‡ à¦†à¦›à§‡ à¦¬à§à¦°à§à¦¯à¦¾à¦¨à§à¦¡ à¦†à¦‡à¦¡à§‡à¦¨à§à¦Ÿà¦¿à¦Ÿà¦¿à¦° à¦¸à¦¾à¦¥à§‡! à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¿ à¦¬à§à¦¯à¦¾à¦— à¦¶à§à¦§à§ à¦à¦•à¦Ÿà¦¿ à¦ªà§à¦¯à¦¾à¦•à§‡à¦œ à¦¨à§Ÿ â€” à¦à¦Ÿà¦¾ à¦†à¦ªà¦¨à¦¾à¦° à¦¬à§à¦°à§à¦¯à¦¾à¦¨à§à¦¡à§‡à¦° à¦šà¦²à¦®à¦¾à¦¨ à¦¬à¦¿à¦œà§à¦žà¦¾à¦ªà¦¨!",
      "contact": "01823549035",
      "delivery_cost_inside": None,
      "delivery_cost_outside": None
    }
  ]

results = []

print("[+] Starting batch Facebook lead analysis...\n")

for i, page in enumerate(pages_to_test, start=1):
    print(f"[{i}/{len(pages_to_test)}] Analyzing: {page['advertiser']} -> {page['advertiser_facebook_link']}")
    try:
        data = analyze_facebook_lead(page["advertiser_facebook_link"], page["advertiser"])
    except Exception as e:
        data = {"probability": 0, "service": None, "reasoning": f"Error: {str(e)}"}
    
    page_result = {
        "page_name": page["name"],
        "url": page["url"],
        **data
    }
    results.append(page_result)
    
    # Sleep a bit to avoid request limits or rate bans
    time.sleep(2)

# --- Save to JSON file ---
with open("output.json", "w", encoding="utf-8") as f:
    json.dump(results, f, ensure_ascii=False, indent=2)

print("\nâœ… Analysis completed. Results saved to 'output.json'.")